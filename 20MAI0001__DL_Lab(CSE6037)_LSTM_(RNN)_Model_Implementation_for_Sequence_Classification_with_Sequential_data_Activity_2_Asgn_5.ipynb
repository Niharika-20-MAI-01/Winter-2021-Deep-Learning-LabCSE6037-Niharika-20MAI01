{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20MAI0001__DL-Lab(CSE6037)_LSTM_(RNN)_Model_Implementation_for_Sequence_Classification_with_Sequential_data_Activity-2_Asgn-5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMawAiCfMcoS5wW1F3ih9BS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niharika-20-MAI-01/Winter-2021-Deep-Learning-LabCSE6037-Niharika-20MAI01/blob/main/20MAI0001__DL_Lab(CSE6037)_LSTM_(RNN)_Model_Implementation_for_Sequence_Classification_with_Sequential_data_Activity_2_Asgn_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saO0zBbXuCvy"
      },
      "source": [
        "#DL-Lab-CSE6037_Lab-Assignment-5_Activity-2 : Implementation of Long Short Term Memory (LSTM) RNN Model for Sequence Classification with Sequential data ---(IMDB (movie review) Dataset )\n",
        "\n",
        "#Submitted By: 20MAI0001 - NIHARIKA MAITRA\n",
        "\n",
        "git Repo Link : \n",
        "\n",
        "https://github.com/Niharika-20-MAI-01/Winter-2021-Deep-Learning-LabCSE6037-Niharika-20MAI01/blob/main/20MAI0001__DL_Lab(CSE6037)_LSTM_(RNN)_Model_Implementation_for_Sequence_Classification_with_Sequential_data_Activity_2_Asgn_5.ipynb\n",
        "\n",
        "\n",
        "https://github.com/Niharika-20-MAI-01/Winter-2021-Deep-Learning-LabCSE6037-Niharika-20MAI01/blob/main/20MAI0001__DL_Lab(CSE6037)_LSTM_(RNN)_Model_Implementation_for_Sequence_Classification_with_Sequential_data_Activity_2_Asgn_5.ipynb\n",
        "\n",
        "https://github.com/Niharika-20-MAI-01/Winter-2021-Deep-Learning-LabCSE6037-Niharika-20MAI01\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO2-F61FtVLT"
      },
      "source": [
        ""
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tBDIqh5ghTK"
      },
      "source": [
        "Sequence classification is a predictive modeling problem where we have some sequence of inputs over space or time and the task is to predict a category for the sequence.\n",
        "\n",
        "#In this case we will demonstrate sequence learning in the IMDB movie review sentiment classification problem by implementing LSTM(RNN) Model for Sequence Classification on the IMDB Sequential dataset :\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRqNo24KLJdS"
      },
      "source": [
        "#About the IMDB dataset : \n",
        "The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly-polar movie reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given movie review has a positive or negative sentiment.\n",
        "\n",
        "Each movie review is a variable sequence of words and the sentiment of each movie review must be classified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xix23Qcii3T"
      },
      "source": [
        "#Pre-processing of the built-in IMDB dataset accesible/uploadable through Keras : \n",
        "The words have been replaced by integers that indicate the ordered frequency of each word in the dataset. The sentences in each review are therefore comprised of a sequence of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmX5tXh_lwF8"
      },
      "source": [
        ""
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W5N9rHht1Ib"
      },
      "source": [
        "# Importing Necessary libraries\n",
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg9U4fk2M3-6"
      },
      "source": [
        ""
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfA2JTGwLyV7"
      },
      "source": [
        "Initializing the random number generator to a constant value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6Cg-lbR6gGM"
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg6Bc8mDM42d"
      },
      "source": [
        ""
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4wbtQfe44YR"
      },
      "source": [
        "#To load the Input Sequential data : IMDB dataset : \n",
        " \n",
        "To load the dataset but only keep the top n words, zero the rest\n",
        "\n",
        "To limit the total number of words that we are interested in modeling to the 5000 most \n",
        "frequent words, and zero out the rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbLyksVQ39dC"
      },
      "source": [
        "# constraining the dataset to the top 5,000 words. \n",
        "top_words = 5000\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R7WB4XmM6BX"
      },
      "source": [
        ""
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m2qZTfCMfFS"
      },
      "source": [
        "# To split the dataset into train (50%) and test (50%) sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4fjLjpa9Cqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee8e3a5-8749-4335-ddc6-537794201e4e"
      },
      "source": [
        "#Keras provides access to the IMDB dataset built-in through the imdb.load_data() function\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW_TTZQSNG-q"
      },
      "source": [
        ""
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJmN3xZM6k9C"
      },
      "source": [
        "# To truncate and pad the input sequences \n",
        "Inorder to make sure  that they are all of the same length for modeling. The model will learn the zero values carry no information so indeed the sequences are not the same length in terms of content, but same length vectors is required to perform the computation in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig3O-Kxck4YY"
      },
      "source": [
        "# To truncate the input sequences :\n",
        "The sequence length (number of words) in each review varies, \n",
        "so to constrain each review to be 500 words, truncating long reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5iDQPtlt1La"
      },
      "source": [
        "max_review_length = 500\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4ccMAfAkvIr"
      },
      "source": [
        ""
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpC3cGQxlQcL"
      },
      "source": [
        "# To pad the input sequences :\n",
        "To pad the shorter reviews with zero values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmeBZgVLDBze"
      },
      "source": [
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbSbPiWGnKTK"
      },
      "source": [
        ""
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXnMb5qzFZKx"
      },
      "source": [
        "# To build /develop the Long Short Term Memory (LSTM) Model : "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzMFGji8Fveh"
      },
      "source": [
        "The Long Short-Term Memory network (LSTM) is a type of Recurrent Neural Network (RNN).\n",
        "By default, an LSTM layer in Keras maintains state between data within one batch. A batch of data is a fixed-sized number of rows from the training dataset that defines how many patterns to process before updating the weights of the network. State in the LSTM layer between batches is cleared by default.\n",
        "\n",
        "The LSTM layer expects input to be in a matrix with the dimensions: [samples, time steps, features].\n",
        "\n",
        "  Samples: These are independent observations from the domain, typically rows of data.\n",
        "\n",
        "  Time steps: These are separate time steps of a given variable for a given observation.\n",
        "  \n",
        "  Features: These are separate measures observed at the time of observation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zVQ4-cFDB2m"
      },
      "source": [
        ""
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcX2DZRpDB45"
      },
      "source": [
        "#To map each word onto a 32 length real valued vector\n",
        "embedding_vecor_length = 32"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sABbmLxoetb9"
      },
      "source": [
        ""
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyhCrRXJO_1l"
      },
      "source": [
        "#To define the LSTM Model being developed / built  :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHGLrwU1OlGn"
      },
      "source": [
        "model_lstm = Sequential()"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2f8f0T2pALg"
      },
      "source": [
        ""
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkwIKvXisRF7"
      },
      "source": [
        "#To add Layers to the LSTM Model being developed / built  :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j85n57CuQA1h"
      },
      "source": [
        "We  map each movie review into a real vector domain, a popular technique when working with text called word embedding. This is a technique where words are encoded as real-valued vectors in a high dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space.\n",
        "\n",
        "Keras provides a convenient way to convert positive integer representations of words into a word embedding by an Embedding layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HsASIhtsNrS"
      },
      "source": [
        "#This first layer is the Embedded layer that uses 32 length vectors to represent each word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf_sd4itPNJp"
      },
      "source": [
        "model_lstm.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqwwWsrypL7m"
      },
      "source": [
        ""
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By0mKiUAfTp8"
      },
      "source": [
        "Recurrent Neural networks like LSTM generally have the problem of overfitting.\n",
        "In order to overcome this issue Dropout can be applied between layers using the Dropout Keras layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8mHqL1FfLh_"
      },
      "source": [
        "#Adding a Dropout layer between Embedding and LSTM layers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMT_TXEifL3V"
      },
      "source": [
        "model_lstm.add(Dropout(0.2))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4jf1xWnpaqs"
      },
      "source": [
        ""
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rD5xmZoO7qx"
      },
      "source": [
        "#The is the LSTM layer with 100 memory units (smart neurons) :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDdm4dUTIMb9"
      },
      "source": [
        "model_lstm.add(LSTM(100))"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZO4_y_npfKq"
      },
      "source": [
        ""
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toVLZBAXfgHJ"
      },
      "source": [
        "#Adding a Dropout layer between LSTM and Dense output layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeF3H18Ufgls"
      },
      "source": [
        "model_lstm.add(Dropout(0.2))"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvakvT5ip0Es"
      },
      "source": [
        ""
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv0INbKaPU7T"
      },
      "source": [
        "# As this is a classification problem : In the Final Layer is used a Dense output layer with a single neuron and a sigmoid activation function to make 0 or 1 predictions for the two classes (good and bad) in this implementation of the LSTM model Sequence Classification on the IMDB dataset :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IopCZdoAIMep"
      },
      "source": [
        "model_lstm.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-scWRvuPqotI"
      },
      "source": [
        ""
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIHoUC6AQpzb"
      },
      "source": [
        "#To Compile the developed / built LSTM Model :\n",
        "#As this is a binary classification problem, in this case Log loss is used as the Loss function (binary_crossentropy in Keras)and the Optimization algorithm as  ADAM Optimizer in this implementation of the LSTM model Sequence Classification on the IMDB dataset :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w4Mj1ULIMg9"
      },
      "source": [
        "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45YrpmXkr1IR"
      },
      "source": [
        ""
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq0pk60iRWjD"
      },
      "source": [
        "#To view the Model Summary of the LSTM Model developed / built :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PNzzXUOIMjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f821a6c-ac2c-4cf1-c764-cfc02260c660"
      },
      "source": [
        "print(model_lstm.summary())"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 500, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRDnIf5Ns1z2"
      },
      "source": [
        ""
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub76HUL-RteE"
      },
      "source": [
        "#To Train the developed / built LSTM Model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmB9wjyDRlqn",
        "outputId": "33be8f88-9fff-4759-a02a-40f7d50b20c2"
      },
      "source": [
        "\n",
        "model_lstm.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "196/196 [==============================] - 12s 52ms/step - loss: 0.6309 - accuracy: 0.6139 - val_loss: 0.3322 - val_accuracy: 0.8612\n",
            "Epoch 2/10\n",
            "196/196 [==============================] - 10s 50ms/step - loss: 0.3741 - accuracy: 0.8457 - val_loss: 0.3173 - val_accuracy: 0.8695\n",
            "Epoch 3/10\n",
            "196/196 [==============================] - 10s 50ms/step - loss: 0.2336 - accuracy: 0.9100 - val_loss: 0.2919 - val_accuracy: 0.8836\n",
            "Epoch 4/10\n",
            "196/196 [==============================] - 10s 49ms/step - loss: 0.1952 - accuracy: 0.9265 - val_loss: 0.2976 - val_accuracy: 0.8815\n",
            "Epoch 5/10\n",
            "196/196 [==============================] - 10s 49ms/step - loss: 0.1865 - accuracy: 0.9318 - val_loss: 0.3130 - val_accuracy: 0.8758\n",
            "Epoch 6/10\n",
            "196/196 [==============================] - 10s 49ms/step - loss: 0.1772 - accuracy: 0.9348 - val_loss: 0.3278 - val_accuracy: 0.8767\n",
            "Epoch 7/10\n",
            "196/196 [==============================] - 10s 49ms/step - loss: 0.1566 - accuracy: 0.9423 - val_loss: 0.3543 - val_accuracy: 0.8682\n",
            "Epoch 8/10\n",
            "196/196 [==============================] - 9s 48ms/step - loss: 0.1432 - accuracy: 0.9480 - val_loss: 0.3516 - val_accuracy: 0.8692\n",
            "Epoch 9/10\n",
            "196/196 [==============================] - 10s 49ms/step - loss: 0.1326 - accuracy: 0.9507 - val_loss: 0.3846 - val_accuracy: 0.8683\n",
            "Epoch 10/10\n",
            "196/196 [==============================] - 10s 49ms/step - loss: 0.1158 - accuracy: 0.9584 - val_loss: 0.4677 - val_accuracy: 0.8602\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5216624a10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFxdQ6dtQtqd"
      },
      "source": [
        ""
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqzaUpDDdP2k"
      },
      "source": [
        "# After Training (fitting) the built LSTM Model with Training data \n",
        "\n",
        "#To estimate the performance of the developed / built LSTM Model on unseen reviews (Test data) of the Input Sequential data: IMDB dataset :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEehOPcJQtyR",
        "outputId": "f3fbc2ac-59c9-497e-d534-0f492d3f2699"
      },
      "source": [
        "# Evaluation of the Accuracy of the developed / built LSTM Model for Implementation on the Test set of the Input Sequential data : IMDB dataset\n",
        "scores = model_lstm.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 86.02%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXBSeERFQt6p"
      },
      "source": [
        ""
      ],
      "execution_count": 78,
      "outputs": []
    }
  ]
}